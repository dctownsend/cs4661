{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Read the data file “Hearts_s.csv” (from github using the following command), and assign it to a Pandas DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "hearts_df = pd.DataFrame()\n",
    "\n",
    "# Read CSV file from given link\n",
    "df = pd.read_csv(\"https://github.com/mpourhoma/CS4661/raw/master/Heart_s.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Check out the dataset. As you see, the dataset contains a number of features including both contextual and biological factors (e.g. age, gender, vital signs, …). The last column “AHD” is the label with “Yes” meaning that a human subject has Heart Disease, and “No” meaning that the subject does not have Heart Disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age Gender     ChestPain  RestBP  Chol  RestECG  MaxHR  Oldpeak  \\\n",
      "0     63      f       typical     145   233        2    150      2.3   \n",
      "1     67      f  asymptomatic     160   286        2    108      1.5   \n",
      "2     67      f  asymptomatic     120   229        2    129      2.6   \n",
      "3     37      f    nonanginal     130   250        0    187      3.5   \n",
      "4     41      m    nontypical     130   204        2    172      1.4   \n",
      "..   ...    ...           ...     ...   ...      ...    ...      ...   \n",
      "296   45      f       typical     110   264        0    132      1.2   \n",
      "297   68      f  asymptomatic     144   193        0    141      3.4   \n",
      "298   57      f  asymptomatic     130   131        0    115      1.2   \n",
      "299   57      m    nontypical     130   236        2    174      0.0   \n",
      "300   38      f    nonanginal     138   175        0    173      0.0   \n",
      "\n",
      "           Thal  AHD  \n",
      "0         fixed   No  \n",
      "1        normal  Yes  \n",
      "2    reversable  Yes  \n",
      "3        normal   No  \n",
      "4        normal   No  \n",
      "..          ...  ...  \n",
      "296  reversable  Yes  \n",
      "297  reversable  Yes  \n",
      "298  reversable  Yes  \n",
      "299      normal  Yes  \n",
      "300      normal   No  \n",
      "\n",
      "[301 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. As you see, there are at least 3 categorical features in the dataset (Gender, ChestPain, Thal). Let’s ignore these categorical features for now, only keep the numerical features and build your feature matrix and label vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62</td>\n",
       "      <td>140</td>\n",
       "      <td>268</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>57</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>63</td>\n",
       "      <td>130</td>\n",
       "      <td>254</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  RestBP  Chol  RestECG  MaxHR  Oldpeak\n",
       "0   63     145   233        2    150      2.3\n",
       "1   67     160   286        2    108      1.5\n",
       "2   67     120   229        2    129      2.6\n",
       "3   37     130   250        0    187      3.5\n",
       "4   41     130   204        2    172      1.4\n",
       "5   56     120   236        0    178      0.8\n",
       "6   62     140   268        2    160      3.6\n",
       "7   57     120   354        0    163      0.6\n",
       "8   63     130   254        2    147      1.4\n",
       "9   53     140   203        2    155      3.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of feature names picked from dataset\n",
    "feature_cols = ['Age', 'RestBP', 'Chol', 'RestECG', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# use this list to select features from the original DataFrame\n",
    "X = df[feature_cols]\n",
    "\n",
    "# prints first 10 rows\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       No\n",
       "10      No\n",
       "20      No\n",
       "30      No\n",
       "40     Yes\n",
       "50      No\n",
       "60     Yes\n",
       "70      No\n",
       "80      No\n",
       "90     Yes\n",
       "100     No\n",
       "110    Yes\n",
       "120    Yes\n",
       "130     No\n",
       "140    Yes\n",
       "150     No\n",
       "160    Yes\n",
       "170     No\n",
       "180    Yes\n",
       "190    Yes\n",
       "200     No\n",
       "210    Yes\n",
       "220     No\n",
       "230    Yes\n",
       "240     No\n",
       "250    Yes\n",
       "260    Yes\n",
       "270    Yes\n",
       "280    Yes\n",
       "290    Yes\n",
       "300     No\n",
       "Name: AHD, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size of Feature Matrix X:\n",
    "# print(X.shape)\n",
    "\n",
    "# select last column label\n",
    "y = df['AHD']\n",
    "\n",
    "# check label vector - print every 10 values\n",
    "y[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Split the dataset into testing and training sets with the following parameters: test_size=0.25, random_state=6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak\n",
      "27    66     150   226        0    114      2.6\n",
      "136   62     120   281        2    103      1.4\n",
      "213   52     112   230        0    160      0.0\n",
      "184   63     140   195        0    179      0.0\n",
      "93    63     135   252        2    172      0.0\n",
      "..   ...     ...   ...      ...    ...      ...\n",
      "192   62     138   294        0    106      1.9\n",
      "212   66     178   228        0    165      1.0\n",
      "112   43     132   341        2    136      3.0\n",
      "100   34     118   182        2    174      0.0\n",
      "13    44     120   263        0    173      0.0\n",
      "\n",
      "[76 rows x 6 columns]\n",
      "\n",
      "\n",
      "27      No\n",
      "136    Yes\n",
      "213    Yes\n",
      "184     No\n",
      "93      No\n",
      "      ... \n",
      "192    Yes\n",
      "212    Yes\n",
      "112    Yes\n",
      "100     No\n",
      "13      No\n",
      "Name: AHD, Length: 76, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# pick 25% of data samples for testing set and 75% for training set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=6)\n",
    "\n",
    "print(X_test)\n",
    "print('\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Use KNN (with k=3), Decision Tree (with random_state=5), and Logistic Regression Classifiers to predict Heart Disease based on the training/testing datasets that you built in part (d). Then check, compare, and report the accuracy of these 3 classifiers. Which one is the best? Which one is the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using method fit training only on training set\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes' 'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No'\n",
      " 'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'Yes'\n",
      " 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No'\n",
      " 'No' 'No' 'No' 'Yes' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No'\n",
      " 'Yes' 'No' 'No' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'No'\n",
      " 'Yes' 'No' 'No' 'Yes' 'Yes' 'Yes' 'No' 'No']\n"
     ]
    }
   ],
   "source": [
    "# testing on testing set:\n",
    "y_predict_knn = knn.predict(X_test)\n",
    "\n",
    "print(y_predict_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6447368421052632\n"
     ]
    }
   ],
   "source": [
    "# knn accuracy evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, y_predict_knn)\n",
    "\n",
    "print(accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# \"my_decisiontree\" is instantiated as an \"object\" of DecisionTreeCLassifier \"class\"\n",
    "#  use random_state=5 for DecisionTree when you initialize it as:\n",
    "#  my_decisiontree = DecisionTreeClassifier(random_state=5)\n",
    "\n",
    "my_decisiontree = DecisionTreeClassifier(random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using method fit training only on training set\n",
    "my_decisiontree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes' 'Yes' 'Yes' 'Yes' 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'No' 'Yes' 'No'\n",
      " 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes'\n",
      " 'Yes' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'Yes'\n",
      " 'Yes' 'Yes' 'Yes' 'No' 'Yes' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'Yes'\n",
      " 'No' 'Yes' 'Yes' 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'No' 'No'\n",
      " 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'Yes' 'No' 'No']\n"
     ]
    }
   ],
   "source": [
    "# testing on the testing set:\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "\n",
    "print(y_predict_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618421052631579\n"
     ]
    }
   ],
   "source": [
    "# decision tree accuracy evaluation\n",
    "accuracy_dt = accuracy_score(y_test, y_predict_dt)\n",
    "\n",
    "print(accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "# decision tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# \"my_decisiontree\" is instantiated as an \"object\" of DecisionTreeCLassifier \"class\"\n",
    "\n",
    "my_logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing on the testing set:\n",
    "my_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'No'\n",
      " 'No' 'Yes' 'Yes' 'No' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'No' 'No' 'No'\n",
      " 'Yes' 'Yes' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'No' 'Yes' 'No' 'Yes' 'Yes'\n",
      " 'Yes' 'Yes' 'No' 'Yes' 'Yes' 'No' 'No' 'Yes' 'Yes' 'Yes' 'Yes' 'No' 'No'\n",
      " 'Yes' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'No' 'Yes' 'Yes' 'No' 'No' 'No'\n",
      " 'Yes' 'Yes' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'No']\n"
     ]
    }
   ],
   "source": [
    "# testing on the testing set:\n",
    "y_predict_lr = my_logreg.predict(X_test)\n",
    "\n",
    "print(y_predict_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6710526315789473\n"
     ]
    }
   ],
   "source": [
    "# logistic regression accuracy evaluation\n",
    "accuracy_lr = accuracy_score(y_test, y_predict_lr)\n",
    "\n",
    "print(accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:                  0.6447368421052632\n",
      "Decision Tree:        0.618421052631579\n",
      "Logistic Regression:  0.6710526315789473\n"
     ]
    }
   ],
   "source": [
    "# compare accuracies\n",
    "print(\"KNN:                 \", accuracy_knn)\n",
    "print(\"Decision Tree:       \", accuracy_dt)\n",
    "print(\"Logistic Regression: \", accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best: logistic regression\n",
    "# worst: decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Now, we want to use the categorical features as well! To this end, we have to perform a feature engineering process called OneHotEncoding for the categorical features. To do this, each categorical feature should be replaced with dummy columns in the feature table (one column for each possible value of a categorical feature), and then encode it in a binary manner such that only one of the dummy columns can take “1” at a time (and zero for the rest). For example, “Gender” can take two values “m” and “f”. Thus, we need to replace this feature (in the feature table) by 2 columns titled “m” and “f”.  Wherever we have a male subject, we can put “1” and ”0” in the columns “m” and “f”.  Wherever we have a female subject, we can put “0” and ”1” in the columns “m” and “f”. (Hint: you will need 4 columns to encode “ChestPain” and 3 columns to encode “Thal”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Gender', 'ChestPain', 'RestBP', 'Chol', 'RestECG', 'MaxHR',\n",
      "       'Oldpeak', 'Thal'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Gender     ChestPain  RestBP  Chol  RestECG  MaxHR  Oldpeak  \\\n",
      "0   63      f       typical     145   233        2    150      2.3   \n",
      "1   67      f  asymptomatic     160   286        2    108      1.5   \n",
      "2   67      f  asymptomatic     120   229        2    129      2.6   \n",
      "3   37      f    nonanginal     130   250        0    187      3.5   \n",
      "4   41      m    nontypical     130   204        2    172      1.4   \n",
      "\n",
      "         Thal  AHD  \n",
      "0       fixed   No  \n",
      "1      normal  Yes  \n",
      "2  reversable  Yes  \n",
      "3      normal   No  \n",
      "4      normal   No  \n",
      "\n",
      "\n",
      "   Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  Gender_f  Gender_m  \\\n",
      "0   63     145   233        2    150      2.3         1         0   \n",
      "1   67     160   286        2    108      1.5         1         0   \n",
      "2   67     120   229        2    129      2.6         1         0   \n",
      "3   37     130   250        0    187      3.5         1         0   \n",
      "4   41     130   204        2    172      1.4         0         1   \n",
      "\n",
      "   ChestPain_asymptomatic  ChestPain_nonanginal  ChestPain_nontypical  \\\n",
      "0                       0                     0                     0   \n",
      "1                       1                     0                     0   \n",
      "2                       1                     0                     0   \n",
      "3                       0                     1                     0   \n",
      "4                       0                     0                     1   \n",
      "\n",
      "   ChestPain_typical  Thal_fixed  Thal_normal  Thal_reversable  AHD  \n",
      "0                  1           1            0                0   No  \n",
      "1                  0           0            1                0  Yes  \n",
      "2                  0           0            0                1  Yes  \n",
      "3                  0           0            1                0   No  \n",
      "4                  0           0            1                0   No  \n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoding using (pd.get_dummies)\n",
    "# categorical features: (gender, chestPain, thal)\n",
    "encoded_df = pd.get_dummies(df, columns = ['Gender', 'ChestPain', 'Thal'])\n",
    "\n",
    "# re-order columns -> AHD is last\n",
    "cols = encoded_df.columns.tolist()\n",
    "cols.append(cols.pop(cols.index('AHD')))\n",
    "encoded_df = encoded_df.reindex(columns = cols)\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(encoded_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. Repeat parts (d) and (e) with the new dataset that you built in part (f). How does the prediction accuracy change for each method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  Gender_f  Gender_m  \\\n",
      "27    66     150   226        0    114      2.6         0         1   \n",
      "136   62     120   281        2    103      1.4         1         0   \n",
      "213   52     112   230        0    160      0.0         1         0   \n",
      "184   63     140   195        0    179      0.0         0         1   \n",
      "93    63     135   252        2    172      0.0         0         1   \n",
      "\n",
      "     ChestPain_asymptomatic  ChestPain_nonanginal  ChestPain_nontypical  \\\n",
      "27                        0                     0                     0   \n",
      "136                       0                     0                     1   \n",
      "213                       1                     0                     0   \n",
      "184                       0                     0                     1   \n",
      "93                        0                     1                     0   \n",
      "\n",
      "     ChestPain_typical  Thal_fixed  Thal_normal  Thal_reversable  \n",
      "27                   1           0            1                0  \n",
      "136                  0           0            0                1  \n",
      "213                  0           0            1                0  \n",
      "184                  0           0            1                0  \n",
      "93                   0           0            1                0  \n",
      "\n",
      "\n",
      "27      No\n",
      "136    Yes\n",
      "213    Yes\n",
      "184     No\n",
      "93      No\n",
      "Name: AHD, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# D. Split the dataset into testing and training sets with the following parameters:\n",
    "# test_size=0.25 & random_state=6\n",
    "\n",
    "# pick 25% of data samples for testing set and 75% for training set\n",
    "# feature_cols includes all columns excluding AHD\n",
    "feature_cols = cols[:-1]\n",
    "X = encoded_df[feature_cols]\n",
    "y = encoded_df['AHD']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=6)\n",
    "\n",
    "print(X_test.head())\n",
    "print('\\n')\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6447368421052632\n"
     ]
    }
   ],
   "source": [
    "# E. Use KNN (with k=5), Decision Tree, and Logistic Regression Classifiers \n",
    "# to predict Heart Disease based on the training/testing datasets that you built in part (d)\n",
    "# Then check, compare, and report the accuracy of these 3 classifiers. Which one is the best? \n",
    "# Which one is the worst?\n",
    "\n",
    "\n",
    "# using method fit training only on training set\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set\n",
    "y_predict_knn = knn.predict(X_test)\n",
    "\n",
    "# KNN accuracy evaluation\n",
    "accuracy_knn = accuracy_score(y_test, y_predict_knn)\n",
    "\n",
    "print(accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "# using method fit training only on training set\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "\n",
    "# decision tree accuracy evaluation\n",
    "accuracy_dt = accuracy_score(y_test, y_predict_dt)\n",
    "\n",
    "print(accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7763157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# using method fit training only on training set\n",
    "my_logreg.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set\n",
    "y_predict_lr = my_logreg.predict(X_test)\n",
    "\n",
    "# logistic regression accuracy evaluation\n",
    "accuracy_lr = accuracy_score(y_test, y_predict_lr)\n",
    "\n",
    "print(accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:                  0.6447368421052632\n",
      "Decision Tree:        0.7368421052631579\n",
      "Logistic Regression:  0.7763157894736842\n"
     ]
    }
   ],
   "source": [
    "# compare accuracies\n",
    "\n",
    "print(\"KNN:                 \", accuracy_knn)\n",
    "print(\"Decision Tree:       \", accuracy_dt)\n",
    "print(\"Logistic Regression: \", accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN accuracy has not changed at all\n",
    "# both accuracies for decision tree and logistic regression have improved slightly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, repeat part (e) with the new dataset that you built in part (f), but this time using Cross-Validation. Thus, rather than splitting the dataset into testing and training, use 10-fold Cross-Validation (as we learned in Lab4) to evaluate the classification methods and report the final prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 15)\n",
      "(301,)\n"
     ]
    }
   ],
   "source": [
    "# importing model_selection as cross_validation is deppreciated\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# feature matrix\n",
    "X = encoded_df[feature_cols]\n",
    "\n",
    "# label vector\n",
    "y = encoded_df['AHD']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70967742 0.63333333 0.56666667 0.66666667 0.6        0.5\n",
      " 0.66666667 0.7        0.56666667 0.73333333]\n",
      "\n",
      "final result:\n",
      "0.6343010752688172\n"
     ]
    }
   ],
   "source": [
    "# knn\n",
    "# applying 10-fold cross validation with KNN classifier\n",
    "accuracy_list = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(accuracy_list)\n",
    "\n",
    "# use avg accuracy vals as result\n",
    "knn_cv = accuracy_list.mean()\n",
    "print(\"\\nfinal result:\")\n",
    "print(knn_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77419355 0.76666667 0.8        0.76666667 0.8        0.7\n",
      " 0.7        0.66666667 0.7        0.83333333]\n",
      "\n",
      "final result:\n",
      "0.750752688172043\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "# applying 10-fold cross validation with dt classifier\n",
    "accuracy_list = cross_val_score(my_decisiontree, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(accuracy_list)\n",
    "\n",
    "# use avg accuracy vals as result\n",
    "dt_cv = accuracy_list.mean()\n",
    "print(\"\\nfinal result:\")\n",
    "print(dt_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77419355 0.76666667 0.8        0.93333333 0.93333333 0.7\n",
      " 0.76666667 0.8        0.8        0.83333333]\n",
      "\n",
      "final result:\n",
      "0.810752688172043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dakotatownsend/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "# applying 10-fold cross validation with lr classifier\n",
    "accuracy_list = cross_val_score(my_logreg, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(accuracy_list)\n",
    "\n",
    "\n",
    "# use avg accuracy vals as result\n",
    "lr_cv = accuracy_list.mean()\n",
    "print(\"\\nfinal result:\")\n",
    "print(lr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:                  0.6343010752688172\n",
      "Decision Tree:        0.750752688172043\n",
      "Logistic Regression:  0.810752688172043\n"
     ]
    }
   ],
   "source": [
    "# comparing accuracies\n",
    "print(\"KNN:                 \", knn_cv)\n",
    "print(\"Decision Tree:       \", dt_cv)\n",
    "print(\"Logistic Regression: \", lr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
